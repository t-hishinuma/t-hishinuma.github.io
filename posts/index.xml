<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>blog on hpc::numa.blog()</title>
    <link>https://hishinuma-t.dev/posts/</link>
    <description>Recent content in blog on hpc::numa.blog()</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Copyright (C) 2020 Toshiaki Hishinuma, All rights reserved.</copyright>
    <lastBuildDate>Tue, 15 Dec 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://hishinuma-t.dev/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>混合精度Krylov部分空間法による連立一次方程式の求解について (前編)</title>
      <link>https://hishinuma-t.dev/posts/advent2020/numerical_analysis16/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/advent2020/numerical_analysis16/</guid>
      <description>20201217追記：よくよく読み返してみるとタイトルが「混合精度」なのに話してることが「高精度」だけで精度を混ぜる話なんにもしてないことに気づいたのでこっそり「前編」だったことにしました．後編は今度書きます．
はじめに この記事は数値計算 Advent Calendar 2020の16日目の記事です．
みなさん連立一次方程式解いてますか？
この研究は脳筋マゾヒストが何も分からずにKrylov部分空間法という闇を殴り続けている過程について書きます．
本記事では連立一次方程式解法の1つのカテゴリであるKrylov部分空間法と， Krylov部分空間法を高精度演算で行うことについて扱います．
高精度と聞いて円周率みたいな細かい桁のシミュレーションと思った方, そういう話じゃありません． 本記事ではみなさんが普段やるような1.0e-8とかの近似解を求める場合に高精度演算を使うと良さそうだ．という話をします．
なお，
 Krylov部分空間法の詳しい説明はしませんので詳しいことが知りたい人はSaad大先生の本を読んでください(PDF落ちてます) 作成者はHPCと高精度演算の人なので理論方面の説明はざっくりです このブログでmathjaxを使うのが初で苦戦してます．数式周りは心の目で見てください 最後まで読んでも高精度演算でKrylov部分空間を実装すると便利だなあ！とはなりません．僕の苦しみが共有されるだけです．応用研究に協力してくれる人を募集しています この記事は個人として書いているので所属とは関係ない Qiitaが嫌いなので個人サイトに書いてますが，コメント等あったらTwitterかメールなりで頂けたらと思います  それでは本編を始めます．
 連立一次方程式 $$ Ax = b $$ の求解アルゴリズムは様々なものが提案されています．
(mathjax使えた～よかった～設定疲れた～)
最も有名なものは直接解法に分類されるLU分解 (ガウスの消去法)かと思います．
これはN*Nの行列に対して， \[{2 \over 3}N^3 (後退代入を含む) \] の計算量で求解できる優れたアルゴリズムです．
一方で多くの物理シミュレーションで現れる偏微分方程式などを差分法や有限要素法などを用いて離散化することによって得られる行列は疎行列 (行列の要素がほとんどゼロの行列)です．
疎行列は解析領域を格子分割するなどによって得られ，解析領域を細かく分割することによって離散化誤差を減らし精度の高いシミュレーションを行えますが，行列のサイズは大きくなり求解にかかる時間が増大します．
一般的に離散化により得られる疎行列は格子点数に依存した次元数および各格子点の節点間の接続に依存した非零要素をもつため， このとき1行あたりの非零要素は変わらず，行列の次元数だけが増えていくことが多いため，行列の密度はどんどん減っていきます．
 ※ 1行あたりの非零要素数は元の問題にも依存しますが，多くて100くらいが一般的に思います (例外はいくらでもありますが)．
そのためN*N行の行列全体の非零要素数は100N程度しかないような行列になります．
※ 実際にこのような行列がどのようにして現れるかについては@nqomorita氏が数値計算Advent Calendar 2018で書かれた 有限要素法と反復法線形ソルバのはじめの一歩[前編]などを読んでください (仕事が明らかに忙しいのは知ってますが後編楽しみに待ってます！)
 話を戻すとこのような疎行列はメモリ使用量を節約するために行番号や列番号のインデックス配列を用いて零要素を記憶しない格納形式を用います (詳しくはCRS形式などを調べてください)．
このような疎行列のメモリレイアウトを用いた場合，後から値を挿入(fill-in)したり列方向にアクセスしたりするのが非常に面倒です．
気軽な気持ちで行列行列積なんかした日には結果として密行列が出てくるので台無しです(疎×疎は疎とは限らない)．
そのため疎行列を係数とする連立一次方程式解法では疎行列の値の書き換えや挿入が頻繁に行われるLU分解などの直接解法でなく反復解法がよく使われます． (なお，マルチフロンタル法など疎行列を疎のままでLU分解する手法もありますが，それについては触れません)
Krylov部分空間法と共役勾配法 Krylov部分空間法は， 連立一次方程式$$Ax = b$$ に対する 初期近似解x_0に対応する初期残差ベクトル $$ r_0 = b - Ax_0$$ を用いて Aのべき乗とr_0の積の像が張る空間から近似解を探索する反復法アルゴリズムの一つのカテゴリで， 行列の性質に応じて様々なアルゴリズムがあります．</description>
    </item>
    
    <item>
      <title>スパコンポエムAdC2020 Day13 SX-Aurora TSUBASAについて酒を飲みながら書いたポエム</title>
      <link>https://hishinuma-t.dev/posts/advent2020/super_con_poem_day13/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/advent2020/super_con_poem_day13/</guid>
      <description>はじめに この記事はスパコンポエムAdC202013日目の記事です．
山田先輩に誘われたはいいものの，私はQiitaのテーマカラーの緑が嫌いなので自分のブログに勝手に書きます．
コメントしたいけどQiitaじゃないからコメントできない！！！と思った人はTwitterにでも書いてください．頑張って発掘します．
今回の記事はSX-Aurora TSUBASA (SXAT)です． SXATはスパコンじゃないだろって？ 何いってんだここにスパコンだって書いてある．
まぁ冗談はさておき東北大のAOBA-AやNIFSの雷神に搭載なので問題ないでしょう． あと次期地球シミュレータにも搭載らしいですね．
この記事ではSXATの演算器構成と，さすがNECとも呼べる自動ベクトル化を最大に活かしたプログラミングモデルについて紹介します． 最後に簡単にSTREAM代わりの内積プログラムの性能について紹介していこうと思います．
いくつかの注意  僕は1ノードまでしか使ったことないので1ノードに限定して書きます 使ってるモデルがType 10-Bというやつなので，東北大や雷神とは動作周波数などが若干違います 明らかに [1] [2]関係者ですが公開情報に基づいて書いています．ただしチャンピオンデータを使った神輿担ぎはしてません この記事は個人として書いているので所属とは関係ない  以上についてご了承ください．
SX-Aurora TSUBASAとは SX-Aurora TSUBASAはシステム全体の名前で，
 ホストとなるマシンをVH (Vector Host), PCIe Gen3接続のアクセラレータボードをVE (Vector Engine)  と呼びます．
↑これ大事
で，VEですが，
これがVEだ！！！！！！！！
＼バーン／
 ～おわり～
制作・著作
━━━━━━
ⒽⓅⒸ
 ↑これは後輩の@recorderさんが作ってくれました．
本題に戻りますが赤いボードだそうです．NECって赤でしたっけ？
半導体に詳しい山田さんはこれを見るだけで演算器構成が透けて見えると思いますが， 僕のような若輩にはまだまだ無理なので，ここからちゃんと調べた結果を紹介します．
ちなみに大体の情報はここにあります．
現状ではVHはIntel CPUのモデルだけのようですが，次期地球シミュレータではEPYCになるらしいので，今後PCIeのカード販売が進むにつれて色々なモデルがでるんじゃないでしょうか．
VEの性能 特徴的なのはVEで，これがいわゆるベクトル機になっています．
Type-10Bと呼ばれるVEの諸元を以下の表に示します．
   Model name Type 10B, 1.4 GHz, 8 core     Peak 2.</description>
    </item>
    
    <item>
      <title>混合精度疎行列計算ライブラリDD-AVX v3のv1.0をリリースした</title>
      <link>https://hishinuma-t.dev/posts/tools/dd-avx_v10/</link>
      <pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/tools/dd-avx_v10/</guid>
      <description>はじめに DD-AVX_v3という名前なのにv1.0とはこれいかに？
ま，まぁとりあえずリリースしたんじゃよ
githubから落としてこられる↓
https://github.com/t-hishinuma/DD-AVX_v3
DD-AVX_v3ってなに？ 一言でいうとSIMDで高速化した倍精度とDouble-Double精度のSparse BLAS．
混合精度のKrylov部分空間法をいかに簡単かつ高速に実装できるかを考えて作ったライブラリ．
悪条件な問題を解こうとしたとき，高精度演算を使うことで丸め誤差による近似解の収束の影響を抑えて早く収束できる．
ただ，例えば行列は他のライブラリから降ってくるので倍精度でよいとか， 残差の計算は別に倍精度でも良いとか，様々な理由ですべてを倍々精度にしなくても良いというシチュエーションがある．
しかし，変数の精度をどうすれば効率的に計算できるのかはよくわからないし， 普通に考えれば独自型と標準型を組み合わせたコードで変数の型をコロコロ変えるのは吐き気がするレベルで面倒くさい．
私は博士に進学する前は高精度なシミュレーションをやりたいと思ったものだが， 倍精度とDDを組み合わせたKrylov部分空間法を簡単に作るのがそもそも面倒臭すぎる ということで始まったのがDD-AVXライブラリで，色々とI/Fを変えているうちにv3になってしまったということである．
DD型を簡単に扱う方法はBailey, Hida先生のQDライブラリの dd_real 型を使うことになるのだが， ベクトルや行列を std::vector&amp;lt;dd_real&amp;gt; のようにして宣言することになる．
このときメモリレイアウト的には下のようなAoS型になる． これが倍精度と組み合わせて混合精度演算をしたり，並列化をしようとしたときに非常に遅くなる．
そこでSoA型にしようとなるのだが，これは std::vector のようなベクトルを簡単に使うI/Fが利用できなくなる．
じゃあSoA型に気合で std::vector っぽい主要機能を全部書いてやればいいやんけ
これが地獄の始まりでした(車輪の再発明ってレベルじゃないぞ1年かかったわ)
ということでDD-AVX_v3では5つのクラスを提供している．
Scalar  d_real (alias of double) dd_real (provided by the QD Library)  Vector  d_real_vector dd_real_vector  Sparse matrix (CRS format)  d_real_SpMat  スカラやベクタの四則演算レベルから基本的な機能を実装してある． これらはD/DDのキャストや代入などもサポートしているため， 連立一次方程式ソルバをtemplateで実装すれば同じI/Fで倍精度とDD型を使える．
現状ではテスト不足で疎行列型はファイルからのI/Oしかできないのと，SpMVしか実装できないです．v1.1でTSpMVと行列I/Fは追加予定
内部ではすべての方の組み合わせに対してAVX / AVX2のintrinsicsを使って最適化してあるため， どの変数を入れてもDDにキャストされるわけではなく，ちゃんと入力に応じたDDのアルゴリズムが呼ばれるようになっている．
ちなみにaxpyの内部実装はこんな感じ↓
https://github.com/t-hishinuma/DD-AVX_v3/blob/master/src/vector/blas/axpy.cpp
具体的にはBLAS Lv.</description>
    </item>
    
    <item>
      <title>scpで圧縮転送，踏み台など</title>
      <link>https://hishinuma-t.dev/posts/tools/scp_command/</link>
      <pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/tools/scp_command/</guid>
      <description>はじめに 以下の記事は昔のブログに書いてて，忘れたら見に行ってたんだけど，昔のブログはそろそろ潰そうかと思っているので，ここに書き直す．
  ファイルを圧縮して高速転送．トラフィックを減らす．
scp -C file usr@name:~/
  暗号化方式を変える(ローカルで暗号化は雑でいいから早く送りたいなど)
scp -c arcfour128 file usr@name:~/
  踏み台経由の転送
scp -o &#39;ProxyCommand ssh 192.168.0.1 -W %h:%p&#39; file user@name:~/
  以上です．おやすみ．</description>
    </item>
    
    <item>
      <title>tmuxでAlt&#43;矢印キーでペインサイズを変更する</title>
      <link>https://hishinuma-t.dev/posts/tools/tmux_arrow/</link>
      <pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/tools/tmux_arrow/</guid>
      <description>よく忘れるので書いておきます
bind-key -n M-Up resize-pane -U 5 bind-key -n M-Down resize-pane -D 5 bind-key -n M-Left resize-pane -L 5 bind-key -n M-Right resize-pane -R 5 tmuxも色々カスタマイズできるけど，基本的に必須なのはこれくらいですかね．．</description>
    </item>
    
    <item>
      <title>ブラウザからサーバにファイルをuploadするためにdroppyを試す</title>
      <link>https://hishinuma-t.dev/posts/tools/droppy/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/tools/droppy/</guid>
      <description>はじめに サーバ上で作業をしていて，手元のファイルをupload / downloadしたいことがよくあると思う．
大きいファイルを1回送るくらいならやるが， たとえばgnuplotやLaTex, Doxygenと格闘しているときなんかは サーバ上で生成された画像をdownloadしたり，手元でAdobeなどで作った画像を何度もUploadするようなシチュエーションが生まれる．
基本的に全てのマシンに同じ環境を構築したいので，マウントするなどのOS依存っぽいことや， 謎のフリーソフトを全部の端末に入れて回るみたいなことはやりたくない． (手動だし)
サーバ側ではコマンド一発で構築できて，クライアント側では導入作業なく利用できるファイルのdonwload/uploadはないものかと思い， dockerで構築できて，ブラウザから利用できる物を探した．
利用は作業中だけの一過性のものなので，セキュリティとかは気にしなくて良いことにする．
Download, 閲覧はあんまり問題にならなくて，httpサーバを立てればいいので
 nginxをdockerであげてみたり (dockerhub) cargoで入れられてディレクトリを一発で公開できるやつを使ったり (basic-http-server)  他にもやりようはいくらでもある．
ちなみに2.は
cargo install basic-http-server basic-http-server -x -a 0.0.0.0:8080&amp;quot; で現在いるディレクトリがhttpで公開されてこんな風に見える
問題はuploadである．特に↑の画像みたいなスクリーンショットをmarkdownに貼るなんてやろうと思った日には スクリーンショットをファイルに保存してターミナル開いてscpするとかしないといけないので発狂しそうになる．
droppy 結論から言うとdroppyというのが軽くて良さそうだということが分かった．
これ自体はnpmのライブラリなのでnode.jsを入れなきゃいけないかと思ったらちゃんとdockerが公開されていた． なんとdocker imagesで確認すると99.5 MBしかない．
root権限でuploadなんかされたらたまったもんじゃないので，ユーザをちゃんと指定してあげることにして，次のようにすればOK． configのフォルダをマウントしてオプションを色々設定できるみたいだけど，ディレクトリ一覧が見れてちょっとuploadできればいいのでデフォルトに任せた．
docker run -p 8080:8989 -v $PWD:/files/ -e UID=$(id -u) -e GID=$(id -g) silverwind/droppy 見た目はこんな感じ．画面分割もできる． 上げるとユーザ名を聞かれるが，適当なユーザ名とパスワードを入れるとログインできる． aとかbとかでいい．真面目にやる場合はconfigで設定するんだと思う．
いいところ
 ドラッグアンドドロップでファイルやディレクトリを送れる スクリーンショットを取ってペーストボタンを押すとちゃんとファイルとして保存される (素晴らしい！) 画像などはブラウザ上で閲覧できる テキストファイルなどはエディタが出てきて編集できる ディレクトリ作成，ファイル削除，リネームなどの簡単な作業ができる docker imagesで確認すると99.5 MBしかない．  悔しいところ</description>
    </item>
    
    <item>
      <title>gmpの実装とC&#43;&#43;からの利用法，性能について(1) [gmp multi-precision]</title>
      <link>https://hishinuma-t.dev/posts/gmp/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/gmp/</guid>
      <description>はじめに そろそろTwitterのフォロワーが2500人を迎えるそうです． やはりマイナーな世界だけを深堀りするのでなく， もっと大衆受けする活動をしていくべきだと思いました．
そう，つまりマスに訴えかけることでHPC界のヒカキンのような存在になるのです．
では皆が見てくれるような一般的な話題ってなんなのか？
私は考えました．
google analyticsの結果を見るとこのページはPEZYと4倍精度という検索ワードでくる人がほとんどのようですが， そんな一部の研究者しか使わないものでなく，もっとマスに訴えかけるような，そんなテーマを考えました．
選ばれたのはGNUでした
そう．GNUに媚びて生きていこう．
つまり今回のテーマはGMPです．
なんと四則演算やFFTについて触れます．全国民がオオウケ間違いなし!
明日から倍精度を使うのをやめ，楽しい多倍長ライフが始まるはずです！
まぁ真面目に導入すると，2月頃にgccのfloat128の性能について書いたのがそこそこ閲覧数が多かったので， 他の高精度演算ライブラリについてもちょっと書くかー，と思って第2弾としてGMPを取りあげてみるということです．．
実際はもっと前から書いていたのですが，細部が書けずにいて，2500人だしここで公開しようと思い立って今に至ります．
様々な言語で多倍長整数は標準サポートされることも多くなってきましたが， 浮動小数点に関してはほとんど見かけませんし，実装自体も少ないです．
やはり浮動小数点演算は癖が強い(指数部が伸びるのか，仮数部が伸びるのか，どのくらい時間がかかるのか)や， ツールそのものがどういった機能を持っているかが分かりにくいです．
今回はGMPに着目して，機能面や実装面について紹介していきたいと思います．
ただ，GMPにはいろいろな難しい特徴があり，正直私もあまり詳しい事は言えないので，フォロワー2500人記念とかぶち上げておいてあれですが， 今回の記事では私が現状で知っているGMPの実装についてちゃんとまとめ，使い方と性能を簡単に見ることにします． (GMPで論文何件か書いたとは思えないほど内部の実装に自信がないおじさん)
GMPを用いたプログラム GMPはGNU Multi Precision Arithmetic Libraryのことで，float128などが4倍精度固定であるのに対し， GMPは変数の宣言時，またはどこかでデフォルトの仮数部の精度を指定することで任意の精度型を作ることができます．
ただし，仮数部を自動的に伸ばしたり，自動的に選択するような機能はついていません(そんなことをしたらメモリが大変なことに)．
指数部については固定で，仮数部を指定していくことになります． そのため例題などではよく円周率などが扱われる印象があります．
ここではC++版のgmpxx.hを使ってGMPの多倍長浮動小数点型であるmpf_tのC++ Wrapper, mpf_classを使ってみます． C++版を使えば演算子オーバーロードによって比較的簡単に実装することができます．
すべての変数が同じ精度で良い場合は，mpf_set_default_prec()という関数を使って精度を指定すればよいです． 簡単なプログラムは以下のようになります．
#include&amp;lt;gmpxx.h&amp;gt;#include&amp;lt;iostream&amp;gt;int main(){ mpf_set_default_prec(1024); mpf_class a(1.5); // a = 1.5 	mpf_class b = 1.0; mpf_class c = 0.0; c = a + b; std::cout &amp;lt;&amp;lt; c &amp;lt;&amp;lt; std::endl; return 0; } ね？簡単でしょ？</description>
    </item>
    
    <item>
      <title>gcloud cloud sdkを使いやすいように環境を整理した</title>
      <link>https://hishinuma-t.dev/posts/tools/gcp_setup/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/tools/gcp_setup/</guid>
      <description>はじめに ちょっと前にすぐに導入方法とかコマンドを忘れて仕方がないので， bashrcに関数作ってGCPでよく使うコマンドを整理したので，忘れないうちにここに書いておく．
テスト用なんかでマシンを立てて，ペペッとコマンド流したいときにやるための：
 マシン一覧取得 マシン起動・停止 マシン接続 マシン削除  という作業を瞬殺する
バイナリをtar.gzで落としてくるのでちゃんと動くか不安だったが，CygwinとWindows上のVM Ubuntu18.04と実機のCentOS8では動いた．
WSLとMac？しらないけど動くんじゃない
google cloud sdkのinstall くわしいことは公式
yum とか apt でも入るって公式には書いてあるけどgoogleの認証情報を渡すコマンドをシステムに入れるのもなあとか思いながら(たぶん認証情報はユーザの場所に置かれるんだろうけど)，tar.gzがあったのでそっちを使うことにした．
まず落としてきて，
wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-280.0.0-linux-x86_64.tar.gz 入れたら解凍して /bin にパス通す．
sh ./install.sh すればいいって公式には書いてあるけど，どこに入るのかよくわかんないから信じずに自分でパスを通しているポンコツが私です．
gcloud components update するとupdateされるので知らんけどとりあえずやる
初期設定 gcloud init するとgoogleアカウントの認証がはじまる． なんか適当にやってるとURLが出てくるのでコピってブラウザに貼ると認証する．
gcloud compute instances list とかやるとマシンのリストが出てくれば勝ち
使う 上で書いたgcloud compute instances listとか，
gcloud compute instances start とか
やれば良いんだけどコマンドが長くてカロリーの無駄． 新しいマシンを作る設定とかは更に長くてオプションも多いし疲れる．
ってことでbashrcに関数作って纏めた．
alias glist=&#39;gcloud compute instances list&#39; alias gssh_up=&#39;gcloud compute config-ssh&#39; gstart() {gcloud compute instances start $1; gcloud compute config-ssh} gstop() {gcloud compute instances stop $1} gtest_up(){ gcloud compute instances create $1 \ --boot-disk-auto-delete \ --maintenance-policy TERMINATE \ --preemptible \ --zone us-central1-a \ --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \ --custom-cpu 8 \ --custom-memory 16 \ --boot-disk-size 10 \ } gtest_up は与えられた引数の名前のマシンはubuntu20.</description>
    </item>
    
    <item>
      <title>数値計算用のベンチマークを取れるコンテナを作っている (OpenBLAS, cuBLAS, fftw, cufft)</title>
      <link>https://hishinuma-t.dev/posts/numa_benchmark/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/numa_benchmark/</guid>
      <description>TL; DR 検索するとCineBenchやゲーム系のベンチマークとかいう何をやっているのか良くわからん結果ばっかりで数値計算の役に立たないのしかでてこない．
STREAMとDGEMM/SGEMMとFFTをCPUとGPUで回してくれるだけでいいんだ！と思うんだけど，冷静に考えるとそういうライブラリってベンチマーク機能が付いてるわけじゃない (てかSTREAMって使いにくいよね．．．)．
ベンチマーク結果はIntelとかNvidiaも出しているけど，アーキごとに項目が整っているとは言いにくい．
数値計算の結果は少ない人数しか興味がなかったのは昔の話で，最近は機械学習の人とかもBLASとかFFTの結果を知りたいはずだから需要がある気がしてるんだけど，統一されたベンチマークがでてくる気配はない．
ベンチマークは比較に意味があるので多くの人が回してくれることが望ましいけど， 我々みたいに自分でOpenBLASだのcuBLASだとfftwだの落としてビルドして，自分でC++でベンチマークコード書いて，自分でsedとかawkとか叩いてgnuplotで整形できる人ばっかりじゃないはず．
そうだ．コンテナ使って数値計算ライブラリの評価が簡単にやれるやつ作ろう．と思い立ってから早かった手作りベンチマークコンテナ
最近はDockerでGPGPUするのも簡単になったしUbuntu 20.04ではDockerがaptで入るようになったしな！
あ，名前は numa_benchmarkにしました．
NUMerical linear Algebra Benchmarkの略であってhishinumaとは関係ありません．本当です．
で，どうやって使うの コードはここにおいて作ってる．ちゃんとCPUとGPUの両方に対応してる．
https://github.com/t-hishinuma/numa_benchmark
コンテナ内に中にrunってコマンドが仕込んであって，実行するとそれを回してくれるようにした．かんたん．
runするとOpenBLASをビルドしていろんなサイズでベンチマークしてくれる．
いまは dot と gemm のベンチマークだけけど， だいたいコンテナ落とす時間も含めて10分くらいで終わるんじゃないかな． 機能面はこれから増やしていくつもり (STREAMはdotがあるからいいかな．．？いいよね．．？)．
DockerHubにも上げてあるので，サイズとかデフォルト設定でいい人は
docker run hishinumat/numa_benchmark run するだけで簡単にベンチマークしてyaml形式で標準出力に結果を出してくる．
サイズとか色々変えたい人はgitからconfigを落としてきて，書き換えて，configのある場所をdockerにマウントすればそのとおりに実行される．
$PWD のマウントで良ければ make benchmark すればdockerコマンド打たなくてよい
git clone git@github.com:t-hishinuma/numa_benchmark.git # vim benchmark_config # if need to change make benchmark 自分でconfigを食わせた場合は結果はresult/に出てくる． せっかくの仮想化だから，pythonのライブラリなんかもコンテナに混ぜて，yamlを元にmatplotlibでプロットしたpngとhtmlも出力してくれるようにした．
でもコードはPythonなんもわからんおじさんすぎてあまりにも適当．
この辺手伝ってくれる人が居たらとても喜びます
結果はこんな感じで，だいたい欲しい情報はでていると思う．
- {&amp;quot;type&amp;quot; : &amp;quot;blas3&amp;quot;, &amp;quot;func&amp;quot; : &amp;quot;sgemm&amp;quot;, &amp;quot;arch&amp;quot; : &amp;quot;cpu&amp;quot;, &amp;quot;# of threads&amp;quot; : 4, &amp;quot;size&amp;quot; : 200, &amp;quot;time [s]&amp;quot; : 0.</description>
    </item>
    
    <item>
      <title>gccの4倍精度の使い方と性能 [gcc quadruple precision]</title>
      <link>https://hishinuma-t.dev/posts/gcc_quad/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/gcc_quad/</guid>
      <description>2011年，5月のリリースでgcc4.6から4倍精度が入った． 昔のブログで軽く使い方を書いたが他のサイトに情報があまり増えてなかったし，私の記事もできが悪かったのでこっちに移転したので改めてまとめてみた．
使い方 簡単な使い方としては， libquadmathを使うことになる．
__float128 型を宣言して使う．倍精度を代入して使うこともできる． 4倍精度数を表現したい場合には後ろに q をつけて使う．
四則演算子や代入なども使えるが，printfだけはchar型の配列に変換してから使う必要がある． 適当なサンプルは以下のとおり：
#include&amp;lt;quadmath.h&amp;gt;#include&amp;lt;stdio.h&amp;gt;int main(){ char str[128]; __float128 a = 1.2345678901234567890q; __float128 b = 1.234; a = a + b; quadmath_snprintf(str,128,&amp;#34;%.40Qf&amp;#34;,a); printf(&amp;#34;%s&amp;#34;,str); return (0); } $ g++ -lquadmath -m128bit-long-double test.c こんな感じに作れる．I/Oがなければ同じプログラムでtemplate使って共通化できるが， I/Oだけは色々調べたがどうしようもなさそうだった．
性能評価 試しに内積を実装して時間を測ってみた．
コードはここにおいた (リポジトリを移動しました, 2020/05/18)．
出力を外に出せば，普通に倍精度と共通化してtemplateで作れた．
gcpで16コアのHaswellマシンを借りて実行してみた．
あんまり参考にはならないが Intel(R) Xeon(R) CPU @ 2.30GHzとのこと．
gccはgcc version 8.2.1 20180905 (Red Hat 8.2.1-3)
OSはCentOS Linux release 8.0.1905 (Core)\
表 各ベクトルサイズにおける倍精度と4倍精度の実行時間 [ms] (-O0, 最適化オプションなし，16 threads)</description>
    </item>
    
    <item>
      <title>Webサイトをgithub pages &#43; Hugoに移行した話</title>
      <link>https://hishinuma-t.dev/posts/hugo_web/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/hugo_web/</guid>
      <description>TL; DR github pagesとHugoでWebページを移行したぜ！
はじめに 要するにWebページを新しい技術(?)を試しつつ作った．
なんでこんな事を始めたかというと，いま2つのWebページを持っていて，それぞれ:
 筑波大学の学籍番号のWebページで業績整理 はてなブログに色々書いていた  いくつもWebサイトを抱えるのは正直嫌だったのと，大学の方は卒業すると(卒業できれば)消えてしまう． いろいろ調べたり考えたりして新しくしたよっていう話．
はてなブログをProにすることも考えたが，Wordpressのサービスだけを利用するのに月額1008円． そんなのWordpressのSaaSの代金としては信じがたい価格のように思えた (だってconohaとかさくらインターネットで他のことにも使える2 coreくらいのVPSが借りられてしまう)ので， Google Domainsで年額1400円のドメイン取って，github pages (無料) + Hugoで静的Webサイトを作ってみた．
それぞれ簡単に説明すると，HugoはMarkdownからhtmlを生成するツールで， HTMLを直接書かなくていいから楽だというもの． 記事はmarkdownで書いて，全体設定はtomlファイルで指定すると，Hugoコマンドがうまいことhtmlを生成してくれる．
github pagesというのはgithubのトップディレクトリにindex.htmlがあればWebページを公開してくれるというサービスで，独自のドメインも設定できる． 独自のドメインにこだわらなければ完全に無料で使える． そうそう超えないだろうけどgithub pagesにはいくつかの制限がある． 執筆している2020年2月の時点では：
 GitHub Pages サイトには、次の使用制限があります: GitHub Pages ソースリポジトリには、1GB の推奨上限があります。詳しい情報については、「私のディスク容量はいくつですか？」を参照してください。 公開されたGitHub Pagesのサイトは1GB以上であってはなりません。 GitHub Pages サイトには、月当たり 100GB のソフトな帯域幅制限があります。 GitHub Pages サイトには、時間当たり 10 ビルドのソフトな制限があります。
 とのことだが，まぁ基本的にいらぬ心配だろう．
このページは既にMarkdownごと私のgithubに置かれていて， 例えばこのページは src/content/posts/hugo_web.md から生成されて /postsに生成されているはずである． これなら万が一，github pagesの容量や制限がかかったとしても，手元にHTMLはあるのでサーバ借りて移すだけで簡単であろうというわけである．
実際どうなの (正直良くない点) 今のところ悪くはないが，静的HTMLなのでWordpressと比べると当然だけど出来ないことはある．
 Blogにコメント欄などをつけられない 業績一覧などをテーブルにして並び替えたり抽出したりできない ブラウザから記事をかけない 細かい調整やjsとの連携がきつい．  1,2は大きい問題ではあるが，ドメイン代の月100円そこらだけで運用できていて， 3も git push するだけなことを考えると，このくらいは諦めるべきだろう． あと，記事の作りやすい環境はVS codeあたりで構築できる気もする．嫌いだから使わないけど．</description>
    </item>
    
  </channel>
</rss>