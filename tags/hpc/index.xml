<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HPC on hpc::numa.blog()</title>
    <link>https://hishinuma-t.dev/tags/hpc/</link>
    <description>Recent content in HPC on hpc::numa.blog()</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Copyright (C) 2020 Toshiaki Hishinuma, All rights reserved.</copyright>
    <lastBuildDate>Sun, 06 Sep 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://hishinuma-t.dev/tags/hpc/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>混合精度疎行列計算ライブラリDD-AVX v3のv1.0をリリースした</title>
      <link>https://hishinuma-t.dev/posts/tools/dd-avx_v10/</link>
      <pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/tools/dd-avx_v10/</guid>
      <description>はじめに DD-AVX_v3という名前なのにv1.0とはこれいかに？
ま，まぁとりあえずリリースしたんじゃよ
githubから落としてこられる↓
https://github.com/t-hishinuma/DD-AVX_v3
DD-AVX_v3ってなに？ 一言でいうとSIMDで高速化した倍精度とDouble-Double精度のSparse BLAS．
混合精度のKrylov部分空間法をいかに簡単かつ高速に実装できるかを考えて作ったライブラリ．
悪条件な問題を解こうとしたとき，高精度演算を使うことで丸め誤差による近似解の収束の影響を抑えて早く収束できる．
ただ，例えば行列は他のライブラリから降ってくるので倍精度でよいとか， 残差の計算は別に倍精度でも良いとか，様々な理由ですべてを倍々精度にしなくても良いというシチュエーションがある．
しかし，変数の精度をどうすれば効率的に計算できるのかはよくわからないし， 普通に考えれば独自型と標準型を組み合わせたコードで変数の型をコロコロ変えるのは吐き気がするレベルで面倒くさい．
私は博士に進学する前は高精度なシミュレーションをやりたいと思ったものだが， 倍精度とDDを組み合わせたKrylov部分空間法を簡単に作るのがそもそも面倒臭すぎる ということで始まったのがDD-AVXライブラリで，色々とI/Fを変えているうちにv3になってしまったということである．
DD型を簡単に扱う方法はBailey, Hida先生のQDライブラリの dd_real 型を使うことになるのだが， ベクトルや行列を std::vector&amp;lt;dd_real&amp;gt; のようにして宣言することになる．
このときメモリレイアウト的には下のようなAoS型になる． これが倍精度と組み合わせて混合精度演算をしたり，並列化をしようとしたときに非常に遅くなる．
そこでSoA型にしようとなるのだが，これは std::vector のようなベクトルを簡単に使うI/Fが利用できなくなる．
じゃあSoA型に気合で std::vector っぽい主要機能を全部書いてやればいいやんけ
これが地獄の始まりでした(車輪の再発明ってレベルじゃないぞ1年かかったわ)
ということでDD-AVX_v3では5つのクラスを提供している．
Scalar  d_real (alias of double) dd_real (provided by the QD Library)  Vector  d_real_vector dd_real_vector  Sparse matrix (CRS format)  d_real_SpMat  スカラやベクタの四則演算レベルから基本的な機能を実装してある． これらはD/DDのキャストや代入などもサポートしているため， 連立一次方程式ソルバをtemplateで実装すれば同じI/Fで倍精度とDD型を使える．
内部ではすべての方の組み合わせに対してAVX / AVX2のintrinsicsを使って最適化してあるため， どの変数を入れてもDDにキャストされるわけではなく，ちゃんと入力に応じたDDのアルゴリズムが呼ばれるようになっている．
ちなみにaxpyの内部実装はこんな感じ↓
https://github.com/t-hishinuma/DD-AVX_v3/blob/master/src/vector/blas/axpy.cpp
具体的にはBLAS Lv.1のaxpyを様々な精度で呼ぶサンプルを見るとわかりやすい．</description>
    </item>
    
    <item>
      <title>gmpの実装とC&#43;&#43;からの利用法，性能について(1) [gmp multi-precision]</title>
      <link>https://hishinuma-t.dev/posts/gmp/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/gmp/</guid>
      <description>はじめに そろそろTwitterのフォロワーが2500人を迎えるそうです． やはりマイナーな世界だけを深堀りするのでなく， もっと大衆受けする活動をしていくべきだと思いました．
そう，つまりマスに訴えかけることでHPC界のヒカキンのような存在になるのです．
では皆が見てくれるような一般的な話題ってなんなのか？
私は考えました．
google analyticsの結果を見るとこのページはPEZYと4倍精度という検索ワードでくる人がほとんどのようですが， そんな一部の研究者しか使わないものでなく，もっとマスに訴えかけるような，そんなテーマを考えました．
選ばれたのはGNUでした
そう．GNUに媚びて生きていこう．
つまり今回のテーマはGMPです．
なんと四則演算やFFTについて触れます．全国民がオオウケ間違いなし!
明日から倍精度を使うのをやめ，楽しい多倍長ライフが始まるはずです！
まぁ真面目に導入すると，2月頃にgccのfloat128の性能について書いたのがそこそこ閲覧数が多かったので， 他の高精度演算ライブラリについてもちょっと書くかー，と思って第2弾としてGMPを取りあげてみるということです．．
実際はもっと前から書いていたのですが，細部が書けずにいて，2500人だしここで公開しようと思い立って今に至ります．
様々な言語で多倍長整数は標準サポートされることも多くなってきましたが， 浮動小数点に関してはほとんど見かけませんし，実装自体も少ないです．
やはり浮動小数点演算は癖が強い(指数部が伸びるのか，仮数部が伸びるのか，どのくらい時間がかかるのか)や， ツールそのものがどういった機能を持っているかが分かりにくいです．
今回はGMPに着目して，機能面や実装面について紹介していきたいと思います．
ただ，GMPにはいろいろな難しい特徴があり，正直私もあまり詳しい事は言えないので，フォロワー2500人記念とかぶち上げておいてあれですが， 今回の記事では私が現状で知っているGMPの実装についてちゃんとまとめ，使い方と性能を簡単に見ることにします． (GMPで論文何件か書いたとは思えないほど内部の実装に自信がないおじさん)
GMPを用いたプログラム GMPはGNU Multi Precision Arithmetic Libraryのことで，float128などが4倍精度固定であるのに対し， GMPは変数の宣言時，またはどこかでデフォルトの仮数部の精度を指定することで任意の精度型を作ることができます．
ただし，仮数部を自動的に伸ばしたり，自動的に選択するような機能はついていません(そんなことをしたらメモリが大変なことに)．
指数部については固定で，仮数部を指定していくことになります． そのため例題などではよく円周率などが扱われる印象があります．
ここではC++版のgmpxx.hを使ってGMPの多倍長浮動小数点型であるmpf_tのC++ Wrapper, mpf_classを使ってみます． C++版を使えば演算子オーバーロードによって比較的簡単に実装することができます．
すべての変数が同じ精度で良い場合は，mpf_set_default_prec()という関数を使って精度を指定すればよいです． 簡単なプログラムは以下のようになります．
#include&amp;lt;gmpxx.h&amp;gt;#include&amp;lt;iostream&amp;gt;int main(){ mpf_set_default_prec(1024); mpf_class a(1.5); // a = 1.5 	mpf_class b = 1.0; mpf_class c = 0.0; c = a + b; std::cout &amp;lt;&amp;lt; c &amp;lt;&amp;lt; std::endl; return 0; } ね？簡単でしょ？</description>
    </item>
    
    <item>
      <title>gcloud cloud sdkを使いやすいように環境を整理した</title>
      <link>https://hishinuma-t.dev/posts/tools/gcp_setup/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/tools/gcp_setup/</guid>
      <description>はじめに ちょっと前にすぐに導入方法とかコマンドを忘れて仕方がないので， bashrcに関数作ってGCPでよく使うコマンドを整理したので，忘れないうちにここに書いておく．
テスト用なんかでマシンを立てて，ペペッとコマンド流したいときにやるための：
 マシン一覧取得 マシン起動・停止 マシン接続 マシン削除  という作業を瞬殺する
バイナリをtar.gzで落としてくるのでちゃんと動くか不安だったが，CygwinとWindows上のVM Ubuntu18.04と実機のCentOS8では動いた．
WSLとMac？しらないけど動くんじゃない
google cloud sdkのinstall くわしいことは公式
yum とか apt でも入るって公式には書いてあるけどgoogleの認証情報を渡すコマンドをシステムに入れるのもなあとか思いながら(たぶん認証情報はユーザの場所に置かれるんだろうけど)，tar.gzがあったのでそっちを使うことにした．
まず落としてきて，
wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-280.0.0-linux-x86_64.tar.gz 入れたら解凍して /bin にパス通す．
sh ./install.sh すればいいって公式には書いてあるけど，どこに入るのかよくわかんないから信じずに自分でパスを通しているポンコツが私です．
gcloud components update するとupdateされるので知らんけどとりあえずやる
初期設定 gcloud init するとgoogleアカウントの認証がはじまる． なんか適当にやってるとURLが出てくるのでコピってブラウザに貼ると認証する．
gcloud compute instances list とかやるとマシンのリストが出てくれば勝ち
使う 上で書いたgcloud compute instances listとか，
gcloud compute instances start とか
やれば良いんだけどコマンドが長くてカロリーの無駄． 新しいマシンを作る設定とかは更に長くてオプションも多いし疲れる．
ってことでbashrcに関数作って纏めた．
alias glist=&#39;gcloud compute instances list&#39; alias gssh_up=&#39;gcloud compute config-ssh&#39; gstart() {gcloud compute instances start $1; gcloud compute config-ssh} gstop() {gcloud compute instances stop $1} gtest_up(){ gcloud compute instances create $1 \ --boot-disk-auto-delete \ --maintenance-policy TERMINATE \ --preemptible \ --zone us-central1-a \ --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \ --custom-cpu 8 \ --custom-memory 16 \ --boot-disk-size 10 \ } gtest_up は与えられた引数の名前のマシンはubuntu20.</description>
    </item>
    
    <item>
      <title>数値計算用のベンチマークを取れるコンテナを作っている (OpenBLAS, cuBLAS, fftw, cufft)</title>
      <link>https://hishinuma-t.dev/posts/numa_benchmark/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/numa_benchmark/</guid>
      <description>TL; DR 検索するとCineBenchやゲーム系のベンチマークとかいう何をやっているのか良くわからん結果ばっかりで数値計算の役に立たないのしかでてこない．
STREAMとDGEMM/SGEMMとFFTをCPUとGPUで回してくれるだけでいいんだ！と思うんだけど，冷静に考えるとそういうライブラリってベンチマーク機能が付いてるわけじゃない (てかSTREAMって使いにくいよね．．．)．
ベンチマーク結果はIntelとかNvidiaも出しているけど，アーキごとに項目が整っているとは言いにくい．
数値計算の結果は少ない人数しか興味がなかったのは昔の話で，最近は機械学習の人とかもBLASとかFFTの結果を知りたいはずだから需要がある気がしてるんだけど，統一されたベンチマークがでてくる気配はない．
ベンチマークは比較に意味があるので多くの人が回してくれることが望ましいけど， 我々みたいに自分でOpenBLASだのcuBLASだとfftwだの落としてビルドして，自分でC++でベンチマークコード書いて，自分でsedとかawkとか叩いてgnuplotで整形できる人ばっかりじゃないはず．
そうだ．コンテナ使って数値計算ライブラリの評価が簡単にやれるやつ作ろう．と思い立ってから早かった手作りベンチマークコンテナ
最近はDockerでGPGPUするのも簡単になったしUbuntu 20.04ではDockerがaptで入るようになったしな！
あ，名前は numa_benchmarkにしました．
NUMerical linear Algebra Benchmarkの略であってhishinumaとは関係ありません．本当です．
で，どうやって使うの コードはここにおいて作ってる．ちゃんとCPUとGPUの両方に対応してる．
https://github.com/t-hishinuma/numa_benchmark
コンテナ内に中にrunってコマンドが仕込んであって，実行するとそれを回してくれるようにした．かんたん．
runするとOpenBLASをビルドしていろんなサイズでベンチマークしてくれる．
いまは dot と gemm のベンチマークだけけど， だいたいコンテナ落とす時間も含めて10分くらいで終わるんじゃないかな． 機能面はこれから増やしていくつもり (STREAMはdotがあるからいいかな．．？いいよね．．？)．
DockerHubにも上げてあるので，サイズとかデフォルト設定でいい人は
docker run hishinumat/numa_benchmark run するだけで簡単にベンチマークしてyaml形式で標準出力に結果を出してくる．
サイズとか色々変えたい人はgitからconfigを落としてきて，書き換えて，configのある場所をdockerにマウントすればそのとおりに実行される．
$PWD のマウントで良ければ make benchmark すればdockerコマンド打たなくてよい
git clone git@github.com:t-hishinuma/numa_benchmark.git # vim benchmark_config # if need to change make benchmark 自分でconfigを食わせた場合は結果はresult/に出てくる． せっかくの仮想化だから，pythonのライブラリなんかもコンテナに混ぜて，yamlを元にmatplotlibでプロットしたpngとhtmlも出力してくれるようにした．
でもコードはPythonなんもわからんおじさんすぎてあまりにも適当．
この辺手伝ってくれる人が居たらとても喜びます
結果はこんな感じで，だいたい欲しい情報はでていると思う．
- {&amp;quot;type&amp;quot; : &amp;quot;blas3&amp;quot;, &amp;quot;func&amp;quot; : &amp;quot;sgemm&amp;quot;, &amp;quot;arch&amp;quot; : &amp;quot;cpu&amp;quot;, &amp;quot;# of threads&amp;quot; : 4, &amp;quot;size&amp;quot; : 200, &amp;quot;time [s]&amp;quot; : 0.</description>
    </item>
    
    <item>
      <title>gccの4倍精度の使い方と性能 [gcc quadruple precision]</title>
      <link>https://hishinuma-t.dev/posts/gcc_quad/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hishinuma-t.dev/posts/gcc_quad/</guid>
      <description>2011年，5月のリリースでgcc4.6から4倍精度が入った． 昔のブログで軽く使い方を書いたが他のサイトに情報があまり増えてなかったし，私の記事もできが悪かったのでこっちに移転したので改めてまとめてみた．
使い方 簡単な使い方としては， libquadmathを使うことになる．
__float128 型を宣言して使う．倍精度を代入して使うこともできる． 4倍精度数を表現したい場合には後ろに q をつけて使う．
四則演算子や代入なども使えるが，printfだけはchar型の配列に変換してから使う必要がある． 適当なサンプルは以下のとおり：
#include&amp;lt;quadmath.h&amp;gt;#include&amp;lt;stdio.h&amp;gt;int main(){ char str[128]; __float128 a = 1.2345678901234567890q; __float128 b = 1.234; a = a + b; quadmath_snprintf(str,128,&amp;#34;%.40Qf&amp;#34;,a); printf(&amp;#34;%s&amp;#34;,str); return (0); } $ g++ -lquadmath -m128bit-long-double test.c こんな感じに作れる．I/Oがなければ同じプログラムでtemplate使って共通化できるが， I/Oだけは色々調べたがどうしようもなさそうだった．
性能評価 試しに内積を実装して時間を測ってみた．
コードはここにおいた (リポジトリを移動しました, 2020/05/18)．
出力を外に出せば，普通に倍精度と共通化してtemplateで作れた．
gcpで16コアのHaswellマシンを借りて実行してみた．
あんまり参考にはならないが Intel(R) Xeon(R) CPU @ 2.30GHzとのこと．
gccはgcc version 8.2.1 20180905 (Red Hat 8.2.1-3)
OSはCentOS Linux release 8.0.1905 (Core)\
表 各ベクトルサイズにおける倍精度と4倍精度の実行時間 [ms] (-O0, 最適化オプションなし，16 threads)</description>
    </item>
    
  </channel>
</rss>